#%%
import torch
import time
import numpy as np
import os
from torch.utils.data import Dataset
from .ch2_augmentations import augment
from .features import get_features
from tqdm import tqdm

class Load_Dataset(Dataset):
    # Initialize your data, download, etc.
    def __init__(self, dataset, config,data_path,training_mode='self_supervised',wh='train'):
        super(Load_Dataset, self).__init__()
        X_train = dataset["samples"][:,0,:].unsqueeze(1)*1000
        y_train = dataset["labels"].long()
        if len(X_train.shape) < 3:
            X_train = X_train.unsqueeze(2)

        if X_train.shape.index(min(X_train.shape)) != 1:  # make sure the Channels in second dim
            X_train = X_train.permute(0, 2, 1)

        if isinstance(X_train, np.ndarray):
            self.x_data = torch.from_numpy(X_train)
        else:
            self.x_data = X_train
        if isinstance(y_train,np.ndarray):
            self.y_data = torch.from_numpy(y_train).long()
        else:
            self.y_data = y_train
        self.len = X_train.shape[0]
        self.config = config

        self.training_mode = training_mode

        for i in range(self.len):
            self.x_data[i,:,375:3375] = torch.nn.functional.interpolate(self.x_data[i].unsqueeze(0),scale_factor=3000/3750)
            self.x_data[i,:,:375] = 0
            self.x_data[i,:,3375:] = 0

        self.x_data = self.x_data[:,:,375:]
        self.x_data = self.x_data[:,:,:3000]


    def __getitem__(self, index):
        if self.training_mode == 'self_supervised':

            weak_dat,strong_dat = augment(self.x_data[index],self.config)

            if isinstance(weak_dat,np.ndarray) :
                weak_dat = torch.from_numpy(weak_dat)

            if isinstance(strong_dat,np.ndarray) :
                strong_dat = torch.from_numpy(strong_dat)

            return weak_dat,strong_dat

        else:
            return self.x_data[index].float(),self.y_data[index]

    def __len__(self):
        return self.len


def data_generator(data_path, configs):

    train_dataset = torch.load(os.path.join(data_path, "pretext.pt"))

    train_dataset = Load_Dataset(train_dataset, configs,data_path=data_path)
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,
                                               shuffle=True, drop_last=configs.drop_last,
                                               )

    return train_loader

def ft_data_generator(data_path,configs):
    train_ds = torch.load(os.path.join(data_path, "train.pt"))
    valid_ds = torch.load(os.path.join(data_path, "val.pt"))

    #test_ds['samples'] = test_ds['samples'][:1000]
    #test_ds['labels'] = test_ds['labels'][:1000]

    test_ds = Load_Dataset(valid_ds, configs,data_path=data_path,training_mode='ft',wh='valid')

    test_loader = torch.utils.data.DataLoader(dataset=test_ds, batch_size=configs.batch_size,
                                               shuffle=False, drop_last=configs.drop_last,
                                               num_workers=10,pin_memory=True,persistent_workers=True)

    train_ds = Load_Dataset(train_ds, configs,data_path=data_path,training_mode='ft',wh='train')
    train_loader = torch.utils.data.DataLoader(dataset=train_ds, batch_size=configs.batch_size,
                                               shuffle=True, drop_last=configs.drop_last,
                                               num_workers=10,pin_memory=True,persistent_workers=True)


    return train_loader,test_loader

def cross_data_generator(data_path,train_idxs,val_idxs,configs):
    train_ds = torch.load(os.path.join(data_path, "train.pt"))
    train_ds['samples'] = train_ds['samples'][:,0,:].unsqueeze(1)
    valid_ds = torch.load(os.path.join(data_path, "val.pt"))
    valid_ds['samples'] = valid_ds['samples'][:,0,:].unsqueeze(1)

    train_subs = ["shhs1-201917.npz","shhs1-203699.npz","shhs1-204777.npz","shhs1-202694.npz","shhs1-204500.npz","shhs1-204115.npz","shhs1-205582.npz","shhs1-205071.npz","shhs1-202636.npz","shhs1-200869.npz","shhs1-204775.npz","shhs1-200052.npz","shhs1-200897.npz","shhs1-204879.npz","shhs1-200823.npz","shhs1-203180.npz","shhs1-200895.npz","shhs1-201839.npz","shhs1-203380.npz","shhs1-200783.npz","shhs1-203312.npz","shhs1-200017.npz","shhs1-204611.npz","shhs1-201117.npz","shhs1-200477.npz","shhs1-204111.npz","shhs1-202253.npz","shhs1-204234.npz","shhs1-203845.npz","shhs1-200383.npz",
"shhs1-203514.npz"
]
    train_segs = [968,1034,1008,852 ,1001,1019,927 ,954 ,1053,1067,1056,1009,892 ,1068,1019,983 ,1026,891 ,1053,959 ,1051,927 ,995 ,959 ,959 ,992 ,846 ,899 ,966 ,838 ,962]

    val_subs = ["shhs1-200039.npz","shhs1-200349.npz","shhs1-200631.npz","shhs1-200737.npz","shhs1-200964.npz","shhs1-201021.npz","shhs1-201023.npz","shhs1-201087.npz","shhs1-201153.npz","shhs1-201316.npz","shhs1-201329.npz","shhs1-201748.npz","shhs1-202108.npz","shhs1-202261.npz","shhs1-202409.npz","shhs1-202735.npz","shhs1-202956.npz","shhs1-203027.npz","shhs1-203651.npz","shhs1-203746.npz","shhs1-204051.npz","shhs1-204083.npz","shhs1-204230.npz","shhs1-204299.npz","shhs1-204350.npz","shhs1-204666.npz","shhs1-204751.npz","shhs1-204811.npz","shhs1-204927.npz","shhs1-205148.npz","shhs1-205207.npz","shhs1-205257.npz","shhs1-205451.npz","shhs1-205610.npz"]
    val_segs = [932,959 ,1053,943 ,1056,1067,997 ,897 ,936 ,1053,1031,944 ,1054,912 ,971 ,960 ,979 ,879 ,1005,990 ,1069,1062,985 ,1017,999 ,931 ,1067,920 ,1006,959 ,1019,959 ,1013,
1019,]

    segs = train_segs+val_segs

    if train_idxs !=[]:

        dataset = {}
        train_dataset = {}
        valid_dataset = {}
        dataset['samples'] = torch.from_numpy(np.vstack((train_ds['samples'],valid_ds['samples'])))
        dataset['labels'] = torch.from_numpy(np.hstack((train_ds['labels'],valid_ds['labels'])))

        print(dataset['samples'].shape)
        print(dataset['labels'].shape)

        dataset['samples'] = torch.split(dataset['samples'],segs)
        dataset['labels'] = torch.split(dataset['labels'],segs)
        print('Split Shape',len(dataset['samples']))

        train_dataset['samples'] = [dataset['samples'][i] for i in train_idxs]
        train_dataset['labels'] = [dataset['labels'][i] for i in train_idxs]

        train_dataset['samples'] = torch.cat(train_dataset['samples'])
        train_dataset['labels'] = torch.cat(train_dataset['labels'])
        print('Train Shape',train_dataset['samples'].shape,train_dataset['labels'].shape)
        train_dataset = Load_Dataset(train_dataset, configs,data_path=data_path,training_mode='ft')

        valid_dataset['samples'] = [dataset['samples'][i] for i in val_idxs]
        valid_dataset['labels'] = [dataset['labels'][i] for i in val_idxs]

        valid_dataset['samples'] = torch.cat(valid_dataset['samples'])
        valid_dataset['labels'] = torch.cat(valid_dataset['labels'])
        valid_dataset = Load_Dataset(valid_dataset,configs,data_path=data_path,training_mode='ft')


        train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=configs.batch_size,
                                                   shuffle=True, drop_last=configs.drop_last,
                                                   )

        valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=configs.batch_size,
                                                   shuffle=False, drop_last=configs.drop_last,
                                                   )

        del dataset
        del train_dataset
        del valid_dataset

        return train_loader,valid_loader
    ret = len(val_subs)+len(train_subs)
    del train_ds
    del valid_ds
    return ret
